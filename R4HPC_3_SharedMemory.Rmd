---
title: "Using R on HPC Clusters: Shared Memory Tools"
author: "George Ostrouchov"
institute: "Oak Ridge National Laboratory"
date: "<br><br><br><br><br><br> August 17, 2022 <br><br><span style = 'font-size: 50%;'> Background Image: FRONTIER, First Top500 exascale system, announced June 2022</span>"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "my-theme.css", "widths.css"]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      titleSlideClass: ["right", "inverse"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
xaringanExtra::use_xaringan_extra(c("tile_view", "animate_css", "tachyons"))
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

## Section 1: Environment and Workflow
## Section 2: Parallel Hardware and Software Overview
## <mark>Section 3:</mark> **Shared Memory Tools**
## Section 4: Distributed Memory Tools
---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7shared.jpg)
background-position: bottom
background-size: 90%

# Working with a single node

---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: bottom
background-size: 90%

# fork via mclapply
???
* we begin with `paralel`'s multicore parts
* continue with Foreign language via libraries (OpenBLAS, nvBLAS)
* go to SPMD MPI with collectives

* reverse of history - because we are used to a laptop 
* Distributed - some things are recomputed rather than communicated


---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: top right
background-size: 20%

# Unix `fork`
* A memory efficient parallelism on shared memory devices  

* Copy-on-write: copy page if forked process tries to write  

* R: **parallel** package `mclapply` and friends  
   * Use for numerical sections only
   * Avoid GUI, I/O, and graphics sections
* Convenient for data (not modified)

* Convenient for functional languages like R

* Careful with nested parallelism
  * OpenBLAS takes all cores by default
  * data.table switches to single threaded mode upon fork

.footnote[A deeper discussion of `fork` memory (if you have interest) on [YouTube](https://www.youtube.com/watch?v=8hVLcyBkSXY) by Chris Kanich (UIC)]

---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: top right
background-size: 20%

# Copy-on-write

```{r echo=FALSE, out.height=500, fig.retina=1, fig.align='center'}
knitr::include_graphics("pics/MC/Fork/Slide1.png")
```
???
* All done with pointers
* Memory is in pages
* Processes not aware of each other or other's memory use
* OS is aware of memory use
* 16 forks write = 16 copies of memory

---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: top right
background-size: 20%

# Mapping (Forked) Processes to Cores
* Operating system manages core affinity
* OS tasks compete and core switching occurs frequently
```{r echo=FALSE, fig.retina=1, fig.align='center'}
knitr::include_graphics("pics/MC/Fork/Fork.png")
```
* Works extremely well unless very large new memory allocation is needed by each process
* Forking can be nested but be careful not to oversubscribe available cores
---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: top right
background-size: 20%

# Parallel Drop-in replacements (almost) <br> for `lapply`, `mapply`, and `Map`
<mark>
`mclapply(X, FUN, ...,`
`         mc.preschedule = TRUE, mc.set.seed = TRUE,`
`         mc.silent = FALSE, mc.cores = getOption("mc.cores", 2L),`
`         mc.cleanup = TRUE, mc.allow.recursive = TRUE, affinity.list = NULL)`
</mark>

`mcmapply(FUN, ...,`
`         MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE,`
`         mc.preschedule = TRUE, mc.set.seed = TRUE,`
`         mc.silent = FALSE, mc.cores = getOption("mc.cores", 2L),`
`         mc.cleanup = TRUE, affinity.list = NULL)`

`mcMap(f, ...)`

---
## Hands-on Session 2 - Multicore Random Forest

* Go to `R4HPC/code_2` directory

* Look at the `rf_serial.R` and `rf_mc.R` codes

---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: top right
background-size: 20%

## Hands-on Session 2 - Example Random forest Code
#### Letter recognition data ( $20\,000 \times 17$ )

```{r eval=FALSE}
library(mlbench)
data(LetterRecognition)
```
<small>
```
[,1] lettr capital letter   
[,2] x.box horizontal position of box  
[,3] y.box vertical position of box  
[,4] width width of box   
[,5] high height of box   
[,6] onpix total number of on pixels  
[,7] x.bar mean x of on pixels in box  
[,8] y.bar mean y of on pixels in box  
[,9] x2bar mean x variance  
[,10] y2bar mean y variance  
[,11] xybar mean x y correlation  
[,12] x2ybr mean of x2y  
[,13] xy2br mean of xy2  
[,14] x.ege mean edge count left to right  
[,15] xegvy correlation of x.ege with y  
[,16] y.ege mean edge count bottom to top  
[,17] yegvx correlation of y.ege with x  
```
</small>
---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7fork.jpg)
background-position: top right
background-size: 20%

## Hands-on Session 2 - Random Forest Classification

### Build many decision trees

### Each tree built from

* random subset of variables: subset of columns

* resampled (with replacement) data: same number of rows

### Use their majority votes to classify

---
### Hands-on Session 2 - `R4HPC/code_2/rf_serial.R`
```{r eval=FALSE, code = readLines("code_2/rf_serial.R")}
```

---
### Hands-on Session 2 - `R4HPC/code_2/rf_mc.R`
```{r eval=FALSE, code = readLines("code_2/rf_mc.R")}
```

---
## Hands-on Session 2 - Assignment

Time the random forest code `rf_mc.R` for 1 through 32 cores by modifying the `rf_MACHINE_slurm.sh` script.
---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7libs.jpg)
background-position: bottom
background-size: 90%

# Libraries via compiled language interfaces

---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7libs.jpg)
background-position: top right
background-size: 20%

# R-LAPACK-BLAS

* BLAS: Basic Linear Algebra Subroutines - A matrix multiplication library
  * `%*%`, `crossprod()`, `sweep()`, `scale()`, and many more

* LAPACK: dense and banded matrix decomposition and more
  * `svd()`, `La.svd()`, `prcomp()`, `princomp()`, `qr()`, `solve()`, `chol()`, `norm()`, and many more
  * But not `lm()`, careful with `qr(x, LAPACK = TRUE)`: column pivoting

* Implementations: OpenBLAS, Intel MKL, Nvidia nvBLAS, Apple vecLib, AMD BLIS, Arm Performance Libraries

* **FlexiBLAS**: A BLAS and LAPACK wrapper library with runtime exchangeable backends
   * Great for dynamic core assignment
   * Great for benchmarking implementations

???
* Optimizes algorithm to chip microarchitecture details 
   * memory hierarchies (L1 cache, L2 cache, etc.) and 
   * register vector length
   
* FlexiBLAS Standardization of API for BLAS core control
   * C, C++, R, Python/numpy, Julia

---

## Faster BLAS For Faster R on your Laptop (macOS)

```{r eval=FALSE}
## Default BLAS from Netlib
> x = matrix(rnorm(1e7), nrow = 1e4)
> system.time(crossprod(x))
   user  system elapsed 
  6.752   0.023   6.801 
```
```{r eval=FALSE}
## vecLib (4 cores)
> system.time(crossprod(x))
   user  system elapsed 
  0.420   0.003   0.078 
```
```{r eval=FALSE}
## OpenBLAS (4 cores)
> system.time(crossprod(x))
   user  system elapsed 
  0.457   0.028   0.075 
```
---
## Install FlexiBLAS For BLAS Control on macOS Laptop

* Install Xcode and command line tools
* Install Homebrew: https://brew.sh/
* In a terminal window:
  * `brew install cmake`
  * `brew install openblas`
* cmake needs to be told about OpenBLAS: 
     * `export CMAKE_PREFIX_PATH=/usr/local/opt/openblas:$CMAKE_PREFIX_PATH`
* Install FlexiBLAS: https://www.mpi-magdeburg.mpg.de/projects/flexiblas
  * See Install section in its README.md
* After installation, link to R (terminal window):
  * `ln -sf /usr/local/lib/libflexiblas.dylib /Library/Frameworks/R.framework/Resources/lib/libRblas.dylib`
* In R, `install.packages("flexiblas")` and test if it works:
  * `flexiblas_avail()`
  * `flexiblas_list()`

R can now swap OpenBLAS and APPLE vecLib dynamically. <br>
Dynamically control number of cores used in OpenBLAS. <br>
Control vecLib threads with VECLIB_MAXIMUM_THREADS before starting R.

???
Long Install

Eases transition to cluster if practiced on laptop

Brings more portability to parallel codes

---
## For faster R on your Windows laptop

Assessing R performance with optimized BLAS across three operating systems   [link](https://thomasmcrow.com/blog/2021-08-optimized-blas-in-r/)

<br>

Building R 4+ for Windows with OpenBLAS    [link](https://www.r-bloggers.com/2020/05/building-r-4-for-windows-with-openblas/)

---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7libs.jpg)
background-position: top right
background-size: 20%

## Hands-on Session 3 - FlexiBLAS

`code_3/flexiblas_setup.R`
```{r eval=FALSE, code = readLines("code_3/flexiblas_setup.r", n = 19)}
```

.footnote[
[https://github.com/Enchufa2/r-flexiblas](https://github.com/Enchufa2/r-flexiblas)  
[https://cran.r-project.org/package=flexiblas](https://cran.r-project.org/package=flexiblas)
]
---
background-image: url(pics/Mangalore/ParallelSoftware/Slide7libs.jpg)
background-position: top right
background-size: 20%

# Hands-on Session 3 - FlexiBLAS

* Go to `code_3` directory
* Submit `flexiblas_MACHINE_slurm.sh`
* Examine output to notice:
  * Maximum cores can be slow
  * Optimal cores can depend on matrix size and shape
